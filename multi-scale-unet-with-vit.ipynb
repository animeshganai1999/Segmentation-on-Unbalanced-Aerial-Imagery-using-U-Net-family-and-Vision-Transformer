{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:50.235186Z",
     "iopub.status.busy": "2022-06-05T00:39:50.234564Z",
     "iopub.status.idle": "2022-06-05T00:39:52.294003Z",
     "shell.execute_reply": "2022-06-05T00:39:52.293105Z",
     "shell.execute_reply.started": "2022-06-05T00:39:50.235101Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from random import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from glob import glob\n",
    "import sys\n",
    "import shutil  \n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:52.296724Z",
     "iopub.status.busy": "2022-06-05T00:39:52.29597Z",
     "iopub.status.idle": "2022-06-05T00:39:52.301953Z",
     "shell.execute_reply": "2022-06-05T00:39:52.300226Z",
     "shell.execute_reply.started": "2022-06-05T00:39:52.296674Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"../input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:52.306041Z",
     "iopub.status.busy": "2022-06-05T00:39:52.304937Z",
     "iopub.status.idle": "2022-06-05T00:39:52.893493Z",
     "shell.execute_reply": "2022-06-05T00:39:52.892529Z",
     "shell.execute_reply.started": "2022-06-05T00:39:52.305998Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import random\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "class segDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, training, transform=None):\n",
    "        super(segDataset, self).__init__()\n",
    "        self.root = root\n",
    "        self.training = training\n",
    "        self.transform = transform\n",
    "        self.IMG_NAMES = sorted(glob(self.root + '/*/images/*.jpg'))\n",
    "        self.BGR_classes = {'Water' : [ 41, 169, 226],\n",
    "                            'Land' : [246,  41, 132],\n",
    "                            'Road' : [228, 193, 110],\n",
    "                            'Building' : [152,  16,  60], \n",
    "                            'Vegetation' : [ 58, 221, 254],\n",
    "                            'Unlabeled' : [155, 155, 155]} # in BGR\n",
    "\n",
    "        self.bin_classes = ['Water', 'Land', 'Road', 'Building', 'Vegetation', 'Unlabeled']\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.IMG_NAMES[idx]\n",
    "        mask_path = img_path.replace('images', 'masks').replace('.jpg', '.png')\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        mask = cv2.imread(mask_path)\n",
    "        cls_mask = np.zeros(mask.shape)\n",
    "        cls_mask[mask == self.BGR_classes['Water']] = self.bin_classes.index('Water')\n",
    "        cls_mask[mask == self.BGR_classes['Land']] = self.bin_classes.index('Land')\n",
    "        cls_mask[mask == self.BGR_classes['Road']] = self.bin_classes.index('Road')\n",
    "        cls_mask[mask == self.BGR_classes['Building']] = self.bin_classes.index('Building')\n",
    "        cls_mask[mask == self.BGR_classes['Vegetation']] = self.bin_classes.index('Vegetation')\n",
    "        cls_mask[mask == self.BGR_classes['Unlabeled']] = self.bin_classes.index('Unlabeled')\n",
    "        cls_mask = cls_mask[:,:,0] \n",
    "        if self.training==True:\n",
    "            if self.transform:\n",
    "              image = transforms.functional.to_pil_image(image)\n",
    "              image = self.transform(image)\n",
    "              image = np.array(image)\n",
    "\n",
    "            # 90 degree rotation\n",
    "            if np.random.rand()<0.5:\n",
    "                angle = np.random.randint(4) * 90\n",
    "                image = ndimage.rotate(image,angle,reshape=True)\n",
    "                cls_mask = ndimage.rotate(cls_mask,angle,reshape=True)\n",
    "\n",
    "            # vertical flip\n",
    "            if np.random.rand()<0.5:\n",
    "                image = np.flip(image, 0)\n",
    "                cls_mask = np.flip(cls_mask, 0)\n",
    "            \n",
    "            # horizonal flip\n",
    "            if np.random.rand()<0.5:\n",
    "                image = np.flip(image, 1)\n",
    "                cls_mask = np.flip(cls_mask, 1)\n",
    "\n",
    "        image = cv2.resize(image, (512,512))/255.0\n",
    "        cls_mask = cv2.resize(cls_mask, (512,512)) \n",
    "        image = np.moveaxis(image, -1, 0)\n",
    "\n",
    "        return torch.tensor(image).float(), torch.tensor(cls_mask, dtype=torch.int64)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.IMG_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:52.896116Z",
     "iopub.status.busy": "2022-06-05T00:39:52.895739Z",
     "iopub.status.idle": "2022-06-05T00:39:52.944212Z",
     "shell.execute_reply": "2022-06-05T00:39:52.94336Z",
     "shell.execute_reply.started": "2022-06-05T00:39:52.896078Z"
    }
   },
   "outputs": [],
   "source": [
    "color_shift = transforms.ColorJitter(.1,.1,.1,.1)\n",
    "blurriness = transforms.GaussianBlur(3, sigma=(0.1, 2.0))\n",
    "\n",
    "t = transforms.Compose([color_shift, blurriness])\n",
    "dataset = segDataset('../input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset', training = True, transform= t)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:52.946305Z",
     "iopub.status.busy": "2022-06-05T00:39:52.945659Z",
     "iopub.status.idle": "2022-06-05T00:39:55.160951Z",
     "shell.execute_reply": "2022-06-05T00:39:55.160232Z",
     "shell.execute_reply.started": "2022-06-05T00:39:52.946264Z"
    }
   },
   "outputs": [],
   "source": [
    "d = dataset[1] ## __getitem__ is called\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.moveaxis(d[0].numpy(),0,-1))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(d[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:55.162203Z",
     "iopub.status.busy": "2022-06-05T00:39:55.161861Z",
     "iopub.status.idle": "2022-06-05T00:39:55.444863Z",
     "shell.execute_reply": "2022-06-05T00:39:55.432748Z",
     "shell.execute_reply.started": "2022-06-05T00:39:55.162162Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:55.446517Z",
     "iopub.status.busy": "2022-06-05T00:39:55.446142Z",
     "iopub.status.idle": "2022-06-05T00:39:55.465092Z",
     "shell.execute_reply": "2022-06-05T00:39:55.463859Z",
     "shell.execute_reply.started": "2022-06-05T00:39:55.446481Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "test_num = int(0.1 * len(dataset))\n",
    "print(f'test data : {test_num}')\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset)-test_num, test_num], generator=torch.Generator().manual_seed(101))\n",
    "\n",
    "BACH_SIZE = 4\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BACH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BACH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:55.46724Z",
     "iopub.status.busy": "2022-06-05T00:39:55.466638Z",
     "iopub.status.idle": "2022-06-05T00:39:55.571112Z",
     "shell.execute_reply": "2022-06-05T00:39:55.570018Z",
     "shell.execute_reply.started": "2022-06-05T00:39:55.467198Z"
    }
   },
   "outputs": [],
   "source": [
    "##################################### for GPU ###########################\n",
    "\n",
    "def get_default_device():\n",
    "    # pick the gpu if available\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data,device):\n",
    "    #move tensors to choosen device\n",
    "    if isinstance(data,(list,tuple)):\n",
    "        return [to_device(x,device) for x in data]\n",
    "    return data.to(device,non_blocking = True)\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    # move the batches of the data to our selected device\n",
    "    def __init__(self,dl,device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "device = get_default_device()\n",
    "\n",
    "train_dataloader = DeviceDataLoader(train_dataloader, device)\n",
    "test_dataloader = DeviceDataLoader(test_dataloader, device)\n",
    "\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:55.5809Z",
     "iopub.status.busy": "2022-06-05T00:39:55.580198Z",
     "iopub.status.idle": "2022-06-05T00:39:55.590701Z",
     "shell.execute_reply": "2022-06-05T00:39:55.589711Z",
     "shell.execute_reply.started": "2022-06-05T00:39:55.580858Z"
    }
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:55.598216Z",
     "iopub.status.busy": "2022-06-05T00:39:55.597699Z",
     "iopub.status.idle": "2022-06-05T00:39:56.089824Z",
     "shell.execute_reply": "2022-06-05T00:39:56.088297Z",
     "shell.execute_reply.started": "2022-06-05T00:39:55.598177Z"
    }
   },
   "outputs": [],
   "source": [
    "for i,j in dataset: \n",
    "    print(i.shape) #[3,512,512]\n",
    "    print(j.shape) #[512,512]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:56.093599Z",
     "iopub.status.busy": "2022-06-05T00:39:56.092598Z",
     "iopub.status.idle": "2022-06-05T00:39:56.121427Z",
     "shell.execute_reply": "2022-06-05T00:39:56.120255Z",
     "shell.execute_reply.started": "2022-06-05T00:39:56.093564Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.conv_13 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1)\n",
    "        self.conv_15 = nn.Conv2d(in_channels, mid_channels, kernel_size=5, padding=2)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(2*mid_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv_23 = nn.Conv2d(2*mid_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv_25 = nn.Conv2d(2*mid_channels, out_channels, kernel_size=5, padding=2)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(2*out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.conv_13(x)\n",
    "        out2 = self.conv_15(x)\n",
    "        #do concatenation store result in out\n",
    "        out = torch.cat((out1,out2),dim = 1)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out1 = self.conv_23(out)\n",
    "        out2 = self.conv_25(out)\n",
    "        #do concatenation store result in out\n",
    "        out = torch.cat((out1,out2),dim = 1)\n",
    "        out = self.batch_norm2(out)\n",
    "        out = self.relu2(out)\n",
    "        return out\n",
    "        \n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "    \n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:56.12416Z",
     "iopub.status.busy": "2022-06-05T00:39:56.123342Z",
     "iopub.status.idle": "2022-06-05T00:40:07.314125Z",
     "shell.execute_reply": "2022-06-05T00:40:07.313071Z",
     "shell.execute_reply.started": "2022-06-05T00:39:56.124127Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:40:07.316418Z",
     "iopub.status.busy": "2022-06-05T00:40:07.316033Z",
     "iopub.status.idle": "2022-06-05T00:40:07.352462Z",
     "shell.execute_reply": "2022-06-05T00:40:07.351568Z",
     "shell.execute_reply.started": "2022-06-05T00:40:07.316376Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, dim, depth, heads, mlp_dim, pool = 'cls', channels = 512, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "        x = self.to_latent(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:40:07.356288Z",
     "iopub.status.busy": "2022-06-05T00:40:07.355977Z",
     "iopub.status.idle": "2022-06-05T00:40:07.368862Z",
     "shell.execute_reply": "2022-06-05T00:40:07.367949Z",
     "shell.execute_reply.started": "2022-06-05T00:40:07.356263Z"
    }
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 32)\n",
    "        self.down1 = Down(64, 64)\n",
    "        self.down2 = Down(128, 128)\n",
    "        self.down3 = Down(256, 256)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 512 // factor)\n",
    "        \n",
    "        self.vit = ViT(image_size = 32,patch_size = 8,dim = 2048, depth = 2, heads = 16,mlp_dim = 12,channels = 512) #dim%head=0\n",
    "        self.vit_conv = nn.Conv2d(32,512,kernel_size = 1,padding = 0) #to increase the number of channels\n",
    "        self.vit_linear = nn.Linear(64,1024)\n",
    "        \n",
    "        self.up1 = Up(1024, 256 // factor, bilinear)\n",
    "        self.up2 = Up(512, 128 // factor, bilinear)\n",
    "        self.up3 = Up(256, 64 // factor, bilinear)\n",
    "        self.up4 = Up(128, 32, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4) #[4,512,32,32]\n",
    "        \n",
    "#       applying Vision Transformer\n",
    "        x6 = self.vit(x5) #[4,2048]\n",
    "        x6 = torch.reshape(x6,(-1,32,8,8)) #[4,32,8,8]\n",
    "        x7 = self.vit_conv(x6) #[4,512,8,8]\n",
    "        x8 = self.vit_linear(torch.reshape(x7,(-1,512,64))) #[4,512,1024]\n",
    "        x9 = torch.reshape(x8,(-1,512,32,32))\n",
    "        \n",
    "        x = self.up1(x9, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:40:07.371343Z",
     "iopub.status.busy": "2022-06-05T00:40:07.370868Z",
     "iopub.status.idle": "2022-06-05T00:40:07.386733Z",
     "shell.execute_reply": "2022-06-05T00:40:07.385736Z",
     "shell.execute_reply.started": "2022-06-05T00:40:07.371268Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=-1)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:40:07.389792Z",
     "iopub.status.busy": "2022-06-05T00:40:07.389007Z",
     "iopub.status.idle": "2022-06-05T00:40:07.39826Z",
     "shell.execute_reply": "2022-06-05T00:40:07.39737Z",
     "shell.execute_reply.started": "2022-06-05T00:40:07.389743Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = FocalLoss(gamma=3/4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:40:07.403127Z",
     "iopub.status.busy": "2022-06-05T00:40:07.401583Z",
     "iopub.status.idle": "2022-06-05T00:40:07.408778Z",
     "shell.execute_reply": "2022-06-05T00:40:07.407867Z",
     "shell.execute_reply.started": "2022-06-05T00:40:07.403088Z"
    }
   },
   "outputs": [],
   "source": [
    "def acc(label, predicted):\n",
    "    seg_acc = (y.cpu() == torch.argmax(pred_mask, axis=1).cpu()).sum() / torch.numel(y.cpu())\n",
    "    return seg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:40:07.41064Z",
     "iopub.status.busy": "2022-06-05T00:40:07.409914Z",
     "iopub.status.idle": "2022-06-05T00:40:11.329001Z",
     "shell.execute_reply": "2022-06-05T00:40:11.328127Z",
     "shell.execute_reply.started": "2022-06-05T00:40:07.410603Z"
    }
   },
   "outputs": [],
   "source": [
    "min_loss = torch.tensor(float('inf'))\n",
    "\n",
    "model = to_device(UNet(n_channels=3, n_classes=6, bilinear=True),device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:40:11.330501Z",
     "iopub.status.busy": "2022-06-05T00:40:11.330169Z",
     "iopub.status.idle": "2022-06-05T00:40:11.338253Z",
     "shell.execute_reply": "2022-06-05T00:40:11.337457Z",
     "shell.execute_reply.started": "2022-06-05T00:40:11.330468Z"
    }
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:40:11.340605Z",
     "iopub.status.busy": "2022-06-05T00:40:11.339883Z",
     "iopub.status.idle": "2022-06-05T00:40:11.346382Z",
     "shell.execute_reply": "2022-06-05T00:40:11.345637Z",
     "shell.execute_reply.started": "2022-06-05T00:40:11.340562Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('./saved_models', exist_ok=True)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "N_DATA = len(train_dataset)\n",
    "N_TEST = len(test_dataset)\n",
    "\n",
    "plot_losses = []\n",
    "scheduler_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:40:11.348439Z",
     "iopub.status.busy": "2022-06-05T00:40:11.347793Z",
     "iopub.status.idle": "2022-06-05T00:40:11.357536Z",
     "shell.execute_reply": "2022-06-05T00:40:11.35677Z",
     "shell.execute_reply.started": "2022-06-05T00:40:11.348401Z"
    }
   },
   "outputs": [],
   "source": [
    "#Total number of trainable parameters\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "  # training\n",
    "  model.train()\n",
    "  loss_list = []\n",
    "  acc_list = []\n",
    "  for batch_i, (x, y) in enumerate(train_dataloader):\n",
    "      pred_mask = model(x)  #[4,6,512,512]\n",
    "      loss = criterion(pred_mask, y)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      loss_list.append(loss.cpu().detach().numpy())\n",
    "      acc_list.append(acc(y,pred_mask).numpy())\n",
    "\n",
    "      sys.stdout.write(\n",
    "          \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f (%f)]\"\n",
    "          % (\n",
    "              epoch,\n",
    "              N_EPOCHS,\n",
    "              batch_i,\n",
    "              len(train_dataloader),\n",
    "              loss.cpu().detach().numpy(),\n",
    "              np.mean(loss_list),\n",
    "          )\n",
    "      )\n",
    "  scheduler_counter += 1\n",
    "  # testing\n",
    "  model.eval()\n",
    "  val_loss_list = []\n",
    "  val_acc_list = []\n",
    "  for batch_i, (x, y) in enumerate(test_dataloader):\n",
    "      with torch.no_grad():    \n",
    "          pred_mask = model(x)  \n",
    "      val_loss = criterion(pred_mask, y)\n",
    "      val_loss_list.append(val_loss.cpu().detach().numpy())\n",
    "      val_acc_list.append(acc(y,pred_mask).numpy())\n",
    "    \n",
    "  print(' epoch {} - loss : {:.5f} - acc : {:.2f} - val loss : {:.5f} - val acc : {:.2f}'.format(epoch, \n",
    "                                                                                                 np.mean(loss_list), \n",
    "                                                                                                 np.mean(acc_list), \n",
    "                                                                                                 np.mean(val_loss_list),\n",
    "                                                                                                 np.mean(val_acc_list)))\n",
    "  plot_losses.append([epoch, np.mean(loss_list), np.mean(val_loss_list)])\n",
    "\n",
    "  compare_loss = np.mean(val_loss_list)\n",
    "  is_best = compare_loss < min_loss\n",
    "  if is_best == True:\n",
    "    scheduler_counter = 0\n",
    "    min_loss = min(compare_loss, min_loss)\n",
    "    torch.save(model.state_dict(), './saved_models/unet_epoch_{}_{:.5f}.pt'.format(epoch,np.mean(val_loss_list)))\n",
    "  \n",
    "  if scheduler_counter > 5:\n",
    "    lr_scheduler.step()\n",
    "    print(f\"lowering learning rate to {optimizer.param_groups[0]['lr']}\")\n",
    "    scheduler_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "plot_losses = np.array(plot_losses)\n",
    "plt.plot(plot_losses[:,0], plot_losses[:,1], color='b', linewidth=4)\n",
    "plt.plot(plot_losses[:,0], plot_losses[:,2], color='r', linewidth=4)\n",
    "plt.title('FocalLoss', fontsize=20)\n",
    "plt.xlabel('epoch',fontsize=20)\n",
    "plt.ylabel('loss',fontsize=20)\n",
    "plt.grid()\n",
    "plt.legend(['training', 'validation']) # using a named size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch_i, (x, y) in enumerate(test_dataloader):\n",
    "    for j in range(len(x)):\n",
    "        result = model(x[j:j+1])\n",
    "        mask = torch.argmax(result, axis=1).cpu().detach().numpy()[0]\n",
    "        im = np.moveaxis(x[j].cpu().detach().numpy(), 0, -1).copy()*255\n",
    "        im = im.astype(int)\n",
    "        gt_mask = y[j].cpu()\n",
    "\n",
    "        plt.figure(figsize=(12,12))\n",
    "\n",
    "        plt.subplot(1,3,1)\n",
    "        im = np.moveaxis(x[j].cpu().detach().numpy(), 0, -1).copy()*255\n",
    "        im = im.astype(int)\n",
    "        plt.imshow(im)\n",
    "\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(gt_mask)\n",
    "\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(mask)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y, pred_mask, classes = 6):\n",
    "    precision_list = [];\n",
    "    for i in range(classes):\n",
    "        actual_num = y.cpu() == i\n",
    "        predicted_num = i == torch.argmax(pred_mask, axis=1).cpu()\n",
    "        prec = torch.logical_and(actual_num,predicted_num).sum()/predicted_num.sum()\n",
    "        precision_list.append(prec.numpy().tolist())\n",
    "    return precision_list\n",
    "\n",
    "def recall(y, pred_mask, classes = 6):\n",
    "    recall_list = []\n",
    "    for i in range(classes):\n",
    "        actual_num = y.cpu() == i\n",
    "        predicted_num = i == torch.argmax(pred_mask, axis=1).cpu()\n",
    "        recall_val = torch.logical_and(actual_num, predicted_num).sum() / actual_num.sum()\n",
    "        recall_list.append(recall_val.numpy().tolist())\n",
    "    return recall_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list = []\n",
    "recall_list = []\n",
    "for batch_i, (x, y) in enumerate(test_dataloader):\n",
    "    for j in range(len(x)):\n",
    "        result = model(x[j:j+1])\n",
    "        precision_list.append(precision(y[j],result))\n",
    "        recall_list.append(recall(y[j],result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(precision_list,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(recall_list,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_precision = np.nanmean(precision_list,axis = 0)\n",
    "sum(final_precision[:-1])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_recall = np.nanmean(recall_list,axis = 0)\n",
    "sum(final_recall)/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"aerialImageSegmentation_using_U-net_and_ViT_100epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
