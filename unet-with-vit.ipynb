{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:49.351019Z",
     "iopub.status.busy": "2022-06-05T00:39:49.350773Z",
     "iopub.status.idle": "2022-06-05T00:39:51.383594Z",
     "shell.execute_reply": "2022-06-05T00:39:51.382861Z",
     "shell.execute_reply.started": "2022-06-05T00:39:49.350949Z"
    },
    "id": "t9E_ChUd1JVd"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from random import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from glob import glob\n",
    "import sys\n",
    "import shutil  \n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WNaKKfp1PYq"
   },
   "source": [
    "# download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:51.3869Z",
     "iopub.status.busy": "2022-06-05T00:39:51.385043Z",
     "iopub.status.idle": "2022-06-05T00:39:51.389875Z",
     "shell.execute_reply": "2022-06-05T00:39:51.389254Z",
     "shell.execute_reply.started": "2022-06-05T00:39:51.386869Z"
    },
    "id": "K0gYseCr-xjq",
    "outputId": "93bfed41-6849-44dd-c9d8-04c658a7ed95"
   },
   "outputs": [],
   "source": [
    "filename = \"../input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDRdk-Xq1WpV"
   },
   "source": [
    "# data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:51.393008Z",
     "iopub.status.busy": "2022-06-05T00:39:51.391296Z",
     "iopub.status.idle": "2022-06-05T00:39:51.796392Z",
     "shell.execute_reply": "2022-06-05T00:39:51.795681Z",
     "shell.execute_reply.started": "2022-06-05T00:39:51.392973Z"
    },
    "id": "R6nhF1um1VBZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import random\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "class segDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, training, transform=None):\n",
    "        super(segDataset, self).__init__()\n",
    "        self.root = root\n",
    "        self.training = training\n",
    "        self.transform = transform\n",
    "        self.IMG_NAMES = sorted(glob(self.root + '/*/images/*.jpg'))\n",
    "        self.BGR_classes = {'Water' : [ 41, 169, 226],\n",
    "                            'Land' : [246,  41, 132],\n",
    "                            'Road' : [228, 193, 110],\n",
    "                            'Building' : [152,  16,  60], \n",
    "                            'Vegetation' : [ 58, 221, 254],\n",
    "                            'Unlabeled' : [155, 155, 155]} # in BGR\n",
    "\n",
    "        self.bin_classes = ['Water', 'Land', 'Road', 'Building', 'Vegetation', 'Unlabeled']\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.IMG_NAMES[idx]\n",
    "        mask_path = img_path.replace('images', 'masks').replace('.jpg', '.png')\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        mask = cv2.imread(mask_path)\n",
    "        cls_mask = np.zeros(mask.shape)  \n",
    "        cls_mask[mask == self.BGR_classes['Water']] = self.bin_classes.index('Water')\n",
    "        cls_mask[mask == self.BGR_classes['Land']] = self.bin_classes.index('Land')\n",
    "        cls_mask[mask == self.BGR_classes['Road']] = self.bin_classes.index('Road')\n",
    "        cls_mask[mask == self.BGR_classes['Building']] = self.bin_classes.index('Building')\n",
    "        cls_mask[mask == self.BGR_classes['Vegetation']] = self.bin_classes.index('Vegetation')\n",
    "        cls_mask[mask == self.BGR_classes['Unlabeled']] = self.bin_classes.index('Unlabeled')\n",
    "        cls_mask = cls_mask[:,:,0] \n",
    "\n",
    "        if self.training==True:\n",
    "            if self.transform:\n",
    "              image = transforms.functional.to_pil_image(image)\n",
    "              image = self.transform(image)\n",
    "              image = np.array(image)\n",
    "\n",
    "            # 90 degree rotation\n",
    "            if np.random.rand()<0.5:\n",
    "              angle = np.random.randint(4) * 90\n",
    "              image = ndimage.rotate(image,angle,reshape=True)\n",
    "              cls_mask = ndimage.rotate(cls_mask,angle,reshape=True)\n",
    "\n",
    "            # vertical flip\n",
    "            if np.random.rand()<0.5:\n",
    "              image = np.flip(image, 0)\n",
    "              cls_mask = np.flip(cls_mask, 0)\n",
    "            \n",
    "            # horizonal flip\n",
    "            if np.random.rand()<0.5:\n",
    "              image = np.flip(image, 1)\n",
    "              cls_mask = np.flip(cls_mask, 1)\n",
    "\n",
    "        image = cv2.resize(image, (512,512))/255.0\n",
    "        cls_mask = cv2.resize(cls_mask, (512,512)) \n",
    "        image = np.moveaxis(image, -1, 0)\n",
    "\n",
    "        return torch.tensor(image).float(), torch.tensor(cls_mask, dtype=torch.int64)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.IMG_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:51.798779Z",
     "iopub.status.busy": "2022-06-05T00:39:51.798558Z",
     "iopub.status.idle": "2022-06-05T00:39:51.848707Z",
     "shell.execute_reply": "2022-06-05T00:39:51.848029Z",
     "shell.execute_reply.started": "2022-06-05T00:39:51.798745Z"
    },
    "id": "VCNI4Ap01alL",
    "outputId": "b7e3ed80-54f9-4021-e5ee-51051591cc56"
   },
   "outputs": [],
   "source": [
    "color_shift = transforms.ColorJitter(.1,.1,.1,.1)\n",
    "blurriness = transforms.GaussianBlur(3, sigma=(0.1, 2.0))\n",
    "\n",
    "t = transforms.Compose([color_shift, blurriness])\n",
    "dataset = segDataset('../input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset', training = True, transform= t)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:51.85012Z",
     "iopub.status.busy": "2022-06-05T00:39:51.849893Z",
     "iopub.status.idle": "2022-06-05T00:39:52.804804Z",
     "shell.execute_reply": "2022-06-05T00:39:52.804162Z",
     "shell.execute_reply.started": "2022-06-05T00:39:51.850089Z"
    },
    "id": "HO-ErIX71b-j",
    "outputId": "753863ec-12ff-484a-ebb5-fe4c983c0ef8"
   },
   "outputs": [],
   "source": [
    "d = dataset[1]\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.moveaxis(d[0].numpy(),0,-1))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(d[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:52.805889Z",
     "iopub.status.busy": "2022-06-05T00:39:52.805673Z",
     "iopub.status.idle": "2022-06-05T00:39:52.81627Z",
     "shell.execute_reply": "2022-06-05T00:39:52.815528Z",
     "shell.execute_reply.started": "2022-06-05T00:39:52.805861Z"
    },
    "id": "m1_T8eQ51iRn",
    "outputId": "38cefdf4-a008-492f-e6ea-f61d4a48ddb3"
   },
   "outputs": [],
   "source": [
    "\n",
    "test_num = int(0.1 * len(dataset))\n",
    "print(f'test data : {test_num}')\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset)-test_num, test_num], generator=torch.Generator().manual_seed(101))\n",
    "\n",
    "BACH_SIZE = 4\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BACH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BACH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:52.818317Z",
     "iopub.status.busy": "2022-06-05T00:39:52.817406Z",
     "iopub.status.idle": "2022-06-05T00:39:52.88188Z",
     "shell.execute_reply": "2022-06-05T00:39:52.880953Z",
     "shell.execute_reply.started": "2022-06-05T00:39:52.818281Z"
    }
   },
   "outputs": [],
   "source": [
    "##################################### for GPU ###########################\n",
    "\n",
    "def get_default_device():\n",
    "    # pick the gpu if available\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data,device):\n",
    "    #move tensors to choosen device\n",
    "    if isinstance(data,(list,tuple)):\n",
    "        return [to_device(x,device) for x in data]\n",
    "    return data.to(device,non_blocking = True)\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    # move the batches of the data to our selected device\n",
    "    def __init__(self,dl,device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "device = get_default_device()\n",
    "\n",
    "train_dataloader = DeviceDataLoader(train_dataloader, device)\n",
    "test_dataloader = DeviceDataLoader(test_dataloader, device)\n",
    "\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:52.884012Z",
     "iopub.status.busy": "2022-06-05T00:39:52.883508Z",
     "iopub.status.idle": "2022-06-05T00:39:52.892757Z",
     "shell.execute_reply": "2022-06-05T00:39:52.891963Z",
     "shell.execute_reply.started": "2022-06-05T00:39:52.883975Z"
    }
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y70ECc-y1i4K"
   },
   "source": [
    "# U-net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:52.89481Z",
     "iopub.status.busy": "2022-06-05T00:39:52.894194Z",
     "iopub.status.idle": "2022-06-05T00:39:52.911062Z",
     "shell.execute_reply": "2022-06-05T00:39:52.910299Z",
     "shell.execute_reply.started": "2022-06-05T00:39:52.894717Z"
    },
    "id": "uaugx4aaO9sR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:39:52.914815Z",
     "iopub.status.busy": "2022-06-05T00:39:52.914628Z",
     "iopub.status.idle": "2022-06-05T00:40:02.775357Z",
     "shell.execute_reply": "2022-06-05T00:40:02.774541Z",
     "shell.execute_reply.started": "2022-06-05T00:39:52.914793Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:40:02.77758Z",
     "iopub.status.busy": "2022-06-05T00:40:02.7773Z",
     "iopub.status.idle": "2022-06-05T00:40:02.811694Z",
     "shell.execute_reply": "2022-06-05T00:40:02.810857Z",
     "shell.execute_reply.started": "2022-06-05T00:40:02.777544Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, dim, depth, heads, mlp_dim, pool = 'cls', channels = 512, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "        x = self.to_latent(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:40:02.814816Z",
     "iopub.status.busy": "2022-06-05T00:40:02.813018Z",
     "iopub.status.idle": "2022-06-05T00:40:02.828584Z",
     "shell.execute_reply": "2022-06-05T00:40:02.827862Z",
     "shell.execute_reply.started": "2022-06-05T00:40:02.814777Z"
    }
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        \n",
    "        self.vit = ViT(image_size = 32,patch_size = 8,dim = 2048, depth = 2, heads = 16,mlp_dim = 12,channels = 512) #dim%head=0\n",
    "        self.vit_conv = nn.Conv2d(32,512,kernel_size = 1,padding = 0) #to increase the number of channels\n",
    "        self.vit_linear = nn.Linear(64,1024)\n",
    "        \n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        #applying Vision Transformer\n",
    "        x6 = self.vit(x5) #[4,2048]\n",
    "        x6 = torch.reshape(x6,(-1,32,8,8)) #[4,32,8,8]\n",
    "        x7 = self.vit_conv(x6) #[4,512,8,8]\n",
    "        x8 = self.vit_linear(torch.reshape(x7,(-1,512,64))) #[4,512,1024]\n",
    "        x9 = torch.reshape(x8,(-1,512,32,32))\n",
    "        \n",
    "        x = self.up1(x9, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFXfKc6R7eAk"
   },
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:40:02.830762Z",
     "iopub.status.busy": "2022-06-05T00:40:02.829982Z",
     "iopub.status.idle": "2022-06-05T00:40:02.844413Z",
     "shell.execute_reply": "2022-06-05T00:40:02.843588Z",
     "shell.execute_reply.started": "2022-06-05T00:40:02.830728Z"
    },
    "id": "GSNwIvCl7jCK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=-1)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:40:02.847546Z",
     "iopub.status.busy": "2022-06-05T00:40:02.847219Z",
     "iopub.status.idle": "2022-06-05T00:40:02.855299Z",
     "shell.execute_reply": "2022-06-05T00:40:02.854525Z",
     "shell.execute_reply.started": "2022-06-05T00:40:02.847514Z"
    },
    "id": "OqecWiyVEiNf"
   },
   "outputs": [],
   "source": [
    "criterion = FocalLoss(gamma=3/4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9V6KTXU4wdR"
   },
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:40:02.857117Z",
     "iopub.status.busy": "2022-06-05T00:40:02.856403Z",
     "iopub.status.idle": "2022-06-05T00:40:02.866743Z",
     "shell.execute_reply": "2022-06-05T00:40:02.865984Z",
     "shell.execute_reply.started": "2022-06-05T00:40:02.857083Z"
    },
    "id": "GZqa0-aofjge"
   },
   "outputs": [],
   "source": [
    "def acc(label, predicted):\n",
    "  seg_acc = (y.cpu() == torch.argmax(pred_mask, axis=1).cpu()).sum() / torch.numel(y.cpu())\n",
    "  return seg_acc\n",
    "def precision(y, pred_mask, classes = 6):\n",
    "    precision_list = [];\n",
    "    for i in range(classes):\n",
    "        actual_num = y.cpu() == i\n",
    "        predicted_num = i == torch.argmax(pred_mask, axis=1).cpu()\n",
    "        prec = torch.logical_and(actual_num,predicted_num).sum()/predicted_num.sum()\n",
    "        precision_list.append(prec.numpy().tolist())\n",
    "    return precision_list\n",
    "\n",
    "def recall(y, pred_mask, classes = 6):\n",
    "    recall_list = []\n",
    "    for i in range(classes):\n",
    "        actual_num = y.cpu() == i\n",
    "        predicted_num = i == torch.argmax(pred_mask, axis=1).cpu()\n",
    "        recall_val = torch.logical_and(actual_num, predicted_num).sum() / actual_num.sum()\n",
    "        recall_list.append(recall_val.numpy().tolist())\n",
    "    return recall_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:40:02.868453Z",
     "iopub.status.busy": "2022-06-05T00:40:02.86793Z",
     "iopub.status.idle": "2022-06-05T00:40:06.797955Z",
     "shell.execute_reply": "2022-06-05T00:40:06.797171Z",
     "shell.execute_reply.started": "2022-06-05T00:40:02.868414Z"
    },
    "id": "2URZk5o240xZ"
   },
   "outputs": [],
   "source": [
    "min_loss = torch.tensor(float('inf'))\n",
    "\n",
    "model = UNet(n_channels=3, n_classes=6, bilinear=True).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T00:40:06.799699Z",
     "iopub.status.busy": "2022-06-05T00:40:06.799429Z"
    },
    "id": "4SUA50ca49LE",
    "outputId": "c6c7065d-4af3-4c64-b671-729c9bf7bc32"
   },
   "outputs": [],
   "source": [
    "os.makedirs('./saved_models', exist_ok=True)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "N_DATA = len(train_dataset)\n",
    "N_TEST = len(test_dataset)\n",
    "\n",
    "plot_losses = []\n",
    "scheduler_counter = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "  # training\n",
    "  model.train()\n",
    "  loss_list = []\n",
    "  acc_list = []\n",
    "  for batch_i, (x, y) in enumerate(train_dataloader):\n",
    "\n",
    "      pred_mask = model(x.to(device))  \n",
    "      loss = criterion(pred_mask, y.to(device))\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      loss_list.append(loss.cpu().detach().numpy())\n",
    "      acc_list.append(acc(y,pred_mask).numpy())\n",
    "\n",
    "      sys.stdout.write(\n",
    "          \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f (%f)]\"\n",
    "          % (\n",
    "              epoch,\n",
    "              N_EPOCHS,\n",
    "              batch_i,\n",
    "              len(train_dataloader),\n",
    "              loss.cpu().detach().numpy(),\n",
    "              np.mean(loss_list),\n",
    "          )\n",
    "      )\n",
    "  scheduler_counter += 1\n",
    "  # testing\n",
    "  model.eval()\n",
    "  val_loss_list = []\n",
    "  val_acc_list = []\n",
    "  for batch_i, (x, y) in enumerate(test_dataloader):\n",
    "      with torch.no_grad():    \n",
    "          pred_mask = model(x.to(device))  \n",
    "      val_loss = criterion(pred_mask, y.to(device))\n",
    "      val_loss_list.append(val_loss.cpu().detach().numpy())\n",
    "      val_acc_list.append(acc(y,pred_mask).numpy())\n",
    "    \n",
    "  print(' epoch {} - loss : {:.5f} - acc : {:.2f} - val loss : {:.5f} - val acc : {:.2f}'.format(epoch, \n",
    "                                                                                                 np.mean(loss_list), \n",
    "                                                                                                 np.mean(acc_list), \n",
    "                                                                                                 np.mean(val_loss_list),\n",
    "                                                                                                 np.mean(val_acc_list)))\n",
    "  plot_losses.append([epoch, np.mean(loss_list), np.mean(val_loss_list)])\n",
    "\n",
    "  compare_loss = np.mean(val_loss_list)\n",
    "  is_best = compare_loss < min_loss\n",
    "  if is_best == True:\n",
    "    scheduler_counter = 0\n",
    "    min_loss = min(compare_loss, min_loss)\n",
    "    torch.save(model.state_dict(), './saved_models/unet_epoch_{}_{:.5f}.pt'.format(epoch,np.mean(val_loss_list)))\n",
    "  \n",
    "  if scheduler_counter > 5:\n",
    "    lr_scheduler.step()\n",
    "    print(f\"lowering learning rate to {optimizer.param_groups[0]['lr']}\")\n",
    "    scheduler_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eGs08A69vPO",
    "outputId": "3faa0a22-537e-480a-87c6-f23fd1737219"
   },
   "outputs": [],
   "source": [
    "# plot loss\n",
    "plot_losses = np.array(plot_losses)\n",
    "plt.plot(plot_losses[:,0], plot_losses[:,1], color='b', linewidth=4)\n",
    "plt.plot(plot_losses[:,0], plot_losses[:,2], color='r', linewidth=4)\n",
    "plt.title('FocalLoss', fontsize=20)\n",
    "plt.xlabel('epoch',fontsize=20)\n",
    "plt.ylabel('loss',fontsize=20)\n",
    "plt.grid()\n",
    "plt.legend(['training', 'validation']) # using a named size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z68ALktN6wB7"
   },
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9zquIrq7Afg",
    "outputId": "780a5cb9-5706-4370-b4b6-5e1f96606689"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch_i, (x, y) in enumerate(test_dataloader):\n",
    "    for j in range(len(x)):\n",
    "        result = model(x[j:j+1])\n",
    "        mask = torch.argmax(result, axis=1).cpu().detach().numpy()[0]\n",
    "        im = np.moveaxis(x[j].cpu().detach().numpy(), 0, -1).copy()*255\n",
    "        im = im.astype(int)\n",
    "        gt_mask = y[j].cpu()\n",
    "\n",
    "        plt.figure(figsize=(12,12))\n",
    "\n",
    "        plt.subplot(1,3,1)\n",
    "        im = np.moveaxis(x[j].cpu().detach().numpy(), 0, -1).copy()*255\n",
    "        im = im.astype(int)\n",
    "        plt.imshow(im)\n",
    "\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(gt_mask)\n",
    "\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(mask)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y, pred_mask, classes = 6):\n",
    "    precision_list = [];\n",
    "    for i in range(classes):\n",
    "        actual_num = y.cpu() == i\n",
    "        predicted_num = i == torch.argmax(pred_mask, axis=1).cpu()\n",
    "        prec = torch.logical_and(actual_num,predicted_num).sum()/predicted_num.sum()\n",
    "        precision_list.append(prec.numpy().tolist())\n",
    "    return precision_list\n",
    "\n",
    "def recall(y, pred_mask, classes = 6):\n",
    "    recall_list = []\n",
    "    for i in range(classes):\n",
    "        actual_num = y.cpu() == i\n",
    "        predicted_num = i == torch.argmax(pred_mask, axis=1).cpu()\n",
    "        recall_val = torch.logical_and(actual_num, predicted_num).sum() / actual_num.sum()\n",
    "        recall_list.append(recall_val.numpy().tolist())\n",
    "    return recall_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "gt_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "for batch_i, (x, y) in enumerate(test_dataloader):\n",
    "    for j in range(len(x)):\n",
    "        result = model(x.to(device)[j:j+1])\n",
    "        precision_list.append(precision(y[j],result))\n",
    "        recall_list.append(recall(y[j],result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJjyaNh3mP8F"
   },
   "outputs": [],
   "source": [
    "np.nanmean(precision_list,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0PLeOWCQ5Dj"
   },
   "outputs": [],
   "source": [
    "np.nanmean(recall_list,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZor6rYqQ81z",
    "outputId": "a0d8a2d9-d6bb-493c-f877-b7189bb855b6"
   },
   "outputs": [],
   "source": [
    "final_precision = np.nanmean(precision_list,axis = 0)\n",
    "sum(final_precision[:-1])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_recall = np.nanmean(recall_list,axis = 0)\n",
    "sum(final_recall)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
